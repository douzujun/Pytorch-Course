{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.word-embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwncHpMDWEKEDGKOknwzXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douzujun/Pytorch_Course/blob/master/2.word_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V39RXHJkwpp0",
        "colab_type": "text"
      },
      "source": [
        "# 1. 词向量\n",
        "\n",
        "- 学习词向量的概念\n",
        "- 用Skip-thought模型训练词向量\n",
        "- 学习使用PyTorch dataset和dataloader\n",
        "- 学习定义PyTorch模型\n",
        "- 学习torch.nn中常见的Module\n",
        "    - Embedding\n",
        "- 学习常见的PyTorch operations\n",
        "    - bmm\n",
        "    - logsigmoid\n",
        "- 保存和读取PyTorch模型\n",
        "    \n",
        "\n",
        "训练数据：\n",
        "\n",
        "链接:https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg  密码:v2z5\n",
        "\n",
        "\n",
        "- 在这一份notebook中，我们会（尽可能）尝试复现论文[Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)中训练词向量的方法. \n",
        "\n",
        "我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。\n",
        "\n",
        "这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。\n",
        "\n",
        "以下是一些我们没有实现的细节\n",
        "- subsampling：参考论文section 2.3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKE5xSbwvqQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as tud\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
        "random.seed(53113)\n",
        "np.random.seed(53113)\n",
        "torch.manual_seed(53113)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(53113)\n",
        "    \n",
        "# 设定一些超参数\n",
        "    \n",
        "K = 100          # number of negative samples\n",
        "C = 3           # nearby words threshold\n",
        "NUM_EPOCHS = 2      # The number of epochs of training\n",
        "MAX_VOCAB_SIZE = 30000  # the vocabulary size\n",
        "BATCH_SIZE = 128     # the batch size\n",
        "LEARNING_RATE = 0.2   # the initial learning rate\n",
        "EMBEDDING_SIZE = 100   # 词向量维度\n",
        "       \n",
        "    \n",
        "LOG_FILE = \"word-embedding.log\"\n",
        "\n",
        "# tokenize函数，把一篇文本转化成一个个单词\n",
        "def word_tokenize(text):\n",
        "    return text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNXauXufLZ9t",
        "colab_type": "text"
      },
      "source": [
        "- 从文本文件中读取所有的文字，通过这些文本创建一个vocabulary\n",
        "\n",
        "- 由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词\n",
        "\n",
        "- 我们添加一个UNK单词表示**所有不常见的单词**\n",
        "\n",
        "- 我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG7yx2JO47oc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3e992991-9de1-4bd1-86e6-f3ffb0284216"
      },
      "source": [
        "with open(\"./text8.train.txt\", \"r\") as fin:\n",
        "    text = fin.read()\n",
        "\n",
        "text = [w for w in word_tokenize(text.lower())]\n",
        "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))  # 29999，单词:词频，降序\n",
        "vocab['<unk>'] = len(text) - np.sum(list(vocab.values()))   # print(vocab)\n",
        "idx_to_word = [word for word in vocab.keys()]\n",
        "word_to_idx = {word:i for i, word in enumerate(idx_to_word)}\n",
        "\n",
        "word_counts = np.array([count for count in vocab.values()], dtype=np.float32) # 词频统计\n",
        "word_freqs = word_counts / np.sum(word_counts)\n",
        "word_freqs = word_freqs ** (3./4.)\n",
        "word_freqs = word_freqs / np.sum(word_freqs) # 用来做 negative sampling\n",
        "VOCAB_SIZE = len(idx_to_word)\n",
        "VOCAB_SIZE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9t_nosncdpb",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 实现Dataloader\n",
        "\n",
        "一个dataloader需要以下内容：\n",
        "\n",
        "- 把所有text编码成数字，然后用subsampling预处理这些文字。\n",
        "\n",
        "- 保存vocabulary，单词count，normalized word frequency\n",
        "\n",
        "- 每个iteration sample一个中心词\n",
        "\n",
        "- 根据当前的中心词，返回context单词\n",
        "\n",
        "- 根据中心词sample一些negative单词，返回单词的counts\n",
        "\n",
        "这里有一个好的tutorial介绍如何使用[PyTorch dataloader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "为了使用dataloader，我们需要定义以下两个function:\n",
        "\n",
        "- ```__len__``` function需要返回整个数据集中有多少个item\n",
        "- ```__get__``` 根据给定的index返回一个item\n",
        "\n",
        "有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMRQ7Yfp4_Dy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordEmbeddingDataset(tud.Dataset): # torch.utils.data\n",
        "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
        "        ''' text: a list of words, all text from the training dataset\n",
        "            word_to_idx: the dictionary from word to idx\n",
        "            idx_to_word: idx to word mapping\n",
        "            word_freq: the frequency of each word\n",
        "            word_counts: the word counts\n",
        "        '''        \n",
        "        super(WordEmbeddingDataset, self).__init__()\n",
        "        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE-1) for t in text]\n",
        "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = idx_to_word\n",
        "        self.word_freqs = torch.Tensor(word_freqs)\n",
        "        self.word_counts = torch.Tensor(word_counts)\n",
        "\n",
        "    def __len__(self):\n",
        "        '''\n",
        "        返回整个数据集（所有单词）的长度\n",
        "        '''\n",
        "        return len(self.text_encoded)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        返回以下数据用于训练\n",
        "        - 中心词\n",
        "        - 这个单词附近（positive）单词\n",
        "        - 随机采样的 K个单词作为 negative sample\n",
        "        '''\n",
        "        center_word = self.text_encoded[idx]\n",
        "        # C: nearby words threshold\n",
        "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))\n",
        "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
        "        pos_words = self.text_encoded[pos_indices]\n",
        "        neg_words = torch.multinomial(self.word_freqs, \n",
        "                          K * pos_words.shape[0], True) \n",
        "        # print(center_word.size(), pos_words.size(), neg_words.size())\n",
        "        return center_word, pos_words, neg_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3rk9lNQwNo2",
        "colab_type": "text"
      },
      "source": [
        "创建dataset和dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnQ058WG4_Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, \n",
        "                  word_freqs, word_counts)\n",
        "\n",
        "# dataset[5]\n",
        "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, \n",
        "                shuffle=True, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTk1HGdI4RDL",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 定义Pytorch模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M2kNP6X4_st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        ''' 初始化输出和输出embedding\n",
        "        '''\n",
        "        super(EmbeddingModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        \n",
        "        initrange = 0.5 / self.embed_size\n",
        "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
        "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "        \n",
        "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
        "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "        \n",
        "    def forward(self, input_labels, pos_labels, neg_labels):\n",
        "        '''\n",
        "        input_labels: 中心词, [batch_size]\n",
        "        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]\n",
        "        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]\n",
        "        \n",
        "        return: loss, [batch_size]\n",
        "        '''\n",
        "        \n",
        "        batch_size = input_labels.size(0)\n",
        "        \n",
        "        input_embedding = self.in_embed(input_labels) # B * embed_size\n",
        "        pos_embedding = self.out_embed(pos_labels) # B * (2*C) * embed_size\n",
        "        neg_embedding = self.out_embed(neg_labels) # B * (2*C * K) * embed_size\n",
        "      \n",
        "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze() # B * (2*C)\n",
        "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze() # B * (2*C*K)\n",
        "\n",
        "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
        "        log_neg = F.logsigmoid(log_neg).sum(1) # batch_size\n",
        "       \n",
        "        loss = log_pos + log_neg\n",
        "        \n",
        "        return -loss\n",
        "    \n",
        "    def input_embeddings(self):\n",
        "        return self.in_embed.weight.data.cpu().numpy()\n",
        "        "
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM9Jqazl4W13",
        "colab_type": "text"
      },
      "source": [
        "定义一个模型以及把模型移动到GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dFqiX0h4_p9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
        "if USE_CUDA:\n",
        "    model = model.cuda()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5RlW1P54aWL",
        "colab_type": "text"
      },
      "source": [
        "下面是评估模型的代码，以及训练模型的代码"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ed26oe54_mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(filename, embedding_weights): \n",
        "    if filename.endswith(\".csv\"):\n",
        "        data = pd.read_csv(filename, sep=\",\")\n",
        "    else:\n",
        "        data = pd.read_csv(filename, sep=\"\\t\")\n",
        "    human_similarity = []\n",
        "    model_similarity = []\n",
        "    for i in data.iloc[:, 0:2].index:\n",
        "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
        "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
        "            continue\n",
        "        else:\n",
        "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
        "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
        "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
        "            human_similarity.append(float(data.iloc[i, 2]))\n",
        "\n",
        "    return scipy.stats.spearmanr(human_similarity, model_similarity)# , model_similarity\n",
        "\n",
        "def find_nearest(word):\n",
        "    index = word_to_idx[word]\n",
        "    embedding = embedding_weights[index]\n",
        "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
        "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]]"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wLiBOFW4d-_",
        "colab_type": "text"
      },
      "source": [
        "训练模型：\n",
        "- 模型一般需要训练若干个epoch\n",
        "- 每个epoch我们都把所有的数据分成若干个batch\n",
        "- 把每个batch的输入和输出都包装成cuda tensor\n",
        "- forward pass，通过输入的句子预测每个单词的下一个单词\n",
        "- 用模型的预测和正确的下一个单词计算cross entropy loss\n",
        "- 清空模型当前gradient\n",
        "- backward pass\n",
        "- 更新模型参数\n",
        "- 每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZSdouJ14_RX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8f197cf-d3ce-486e-c110-9405fc25b1f0"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "for e in range(NUM_EPOCHS):\n",
        "    \n",
        "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
        "        \n",
        "        \n",
        "        # TODO\n",
        "        input_labels = input_labels.long()\n",
        "        pos_labels = pos_labels.long()\n",
        "        neg_labels = neg_labels.long()\n",
        "        if USE_CUDA:\n",
        "            input_labels = input_labels.cuda()\n",
        "            pos_labels = pos_labels.cuda()\n",
        "            neg_labels = neg_labels.cuda()\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            with open(LOG_FILE, \"a\") as fout:\n",
        "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
        "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
        "            \n",
        "        \n",
        "        if i % 2000 == 0:\n",
        "            embedding_weights = model.input_embeddings()\n",
        "            sim_simlex = evaluate(\"simlex-999.txt\", embedding_weights)\n",
        "            sim_men = evaluate(\"men.txt\", embedding_weights)\n",
        "            sim_353 = evaluate(\"wordsim353.csv\", embedding_weights)\n",
        "            with open(LOG_FILE, \"a\") as fout:\n",
        "                print(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
        "                    e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
        "                fout.write(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
        "                    e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
        "                \n",
        "    embedding_weights = model.input_embeddings()\n",
        "    np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights)\n",
        "    torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0, iter: 0, loss: 420.04705810546875\n",
            "epoch: 0, iteration: 0, simlex-999: SpearmanrResult(correlation=-0.05282378708920793, pvalue=0.10334533735327008), men: SpearmanrResult(correlation=-0.018215895022414455, pvalue=0.3626038049694902), sim353: SpearmanrResult(correlation=0.0618943367327749, pvalue=0.2749700696031102), nearest to monster: ['monster', 'requests', 'commanded', 'martina', 'balearic', 'progressively', 'irregularity', 'marlin', 'ellsworth', 'coup']\n",
            "\n",
            "epoch: 0, iter: 100, loss: 281.1331787109375\n",
            "epoch: 0, iter: 200, loss: 208.61724853515625\n",
            "epoch: 0, iter: 300, loss: 191.60409545898438\n",
            "epoch: 0, iter: 400, loss: 150.77288818359375\n",
            "epoch: 0, iter: 500, loss: 137.9649200439453\n",
            "epoch: 0, iter: 600, loss: 120.3508529663086\n",
            "epoch: 0, iter: 700, loss: 126.17131805419922\n",
            "epoch: 0, iter: 800, loss: 101.56168365478516\n",
            "epoch: 0, iter: 900, loss: 109.11630249023438\n",
            "epoch: 0, iter: 1000, loss: 101.89788055419922\n",
            "epoch: 0, iter: 1100, loss: 105.05744934082031\n",
            "epoch: 0, iter: 1200, loss: 85.43743896484375\n",
            "epoch: 0, iter: 1300, loss: 83.00949096679688\n",
            "epoch: 0, iter: 1400, loss: 73.96321105957031\n",
            "epoch: 0, iter: 1500, loss: 82.67890930175781\n",
            "epoch: 0, iter: 1600, loss: 72.89862060546875\n",
            "epoch: 0, iter: 1700, loss: 73.42420196533203\n",
            "epoch: 0, iter: 1800, loss: 70.46649932861328\n",
            "epoch: 0, iter: 1900, loss: 58.09662628173828\n",
            "epoch: 0, iter: 2000, loss: 63.34321594238281\n",
            "epoch: 0, iteration: 2000, simlex-999: SpearmanrResult(correlation=0.007698547343319818, pvalue=0.8124812765362197), men: SpearmanrResult(correlation=0.050837744882905135, pvalue=0.011013824071267957), sim353: SpearmanrResult(correlation=-0.06198504334335602, pvalue=0.2742658794617143), nearest to monster: ['monster', 'permitted', 'medium', 'bonds', 'manager', 'sort', 'franklin', 'translations', 'projects', 'feel']\n",
            "\n",
            "epoch: 0, iter: 2100, loss: 65.32840728759766\n",
            "epoch: 0, iter: 2200, loss: 72.20711517333984\n",
            "epoch: 0, iter: 2300, loss: 54.3302116394043\n",
            "epoch: 0, iter: 2400, loss: 64.86758422851562\n",
            "epoch: 0, iter: 2500, loss: 66.84441375732422\n",
            "epoch: 0, iter: 2600, loss: 65.73758697509766\n",
            "epoch: 0, iter: 2700, loss: 48.996498107910156\n",
            "epoch: 0, iter: 2800, loss: 58.60822677612305\n",
            "epoch: 0, iter: 2900, loss: 56.223392486572266\n",
            "epoch: 0, iter: 3000, loss: 50.3648796081543\n",
            "epoch: 0, iter: 3100, loss: 51.476097106933594\n",
            "epoch: 0, iter: 3200, loss: 52.69061279296875\n",
            "epoch: 0, iter: 3300, loss: 48.50078201293945\n",
            "epoch: 0, iter: 3400, loss: 42.325897216796875\n",
            "epoch: 0, iter: 3500, loss: 53.18551254272461\n",
            "epoch: 0, iter: 3600, loss: 51.61853790283203\n",
            "epoch: 0, iter: 3700, loss: 42.68690490722656\n",
            "epoch: 0, iter: 3800, loss: 48.73640823364258\n",
            "epoch: 0, iter: 3900, loss: 49.478233337402344\n",
            "epoch: 0, iter: 4000, loss: 52.147315979003906\n",
            "epoch: 0, iteration: 4000, simlex-999: SpearmanrResult(correlation=0.01656910720873489, pvalue=0.6096347779232151), men: SpearmanrResult(correlation=0.04016831539856388, pvalue=0.04461972713800172), sim353: SpearmanrResult(correlation=-0.041289865470427366, pvalue=0.4666869971746973), nearest to monster: ['monster', 'thrown', 'ammonia', 'artificial', 'abbey', 'strait', 'capability', 'locations', 'properly', 'tail']\n",
            "\n",
            "epoch: 0, iter: 4100, loss: 40.83182144165039\n",
            "epoch: 0, iter: 4200, loss: 40.51793670654297\n",
            "epoch: 0, iter: 4300, loss: 42.939212799072266\n",
            "epoch: 0, iter: 4400, loss: 45.88116455078125\n",
            "epoch: 0, iter: 4500, loss: 49.404014587402344\n",
            "epoch: 0, iter: 4600, loss: 46.82133865356445\n",
            "epoch: 0, iter: 4700, loss: 41.23723220825195\n",
            "epoch: 0, iter: 4800, loss: 38.73188781738281\n",
            "epoch: 0, iter: 4900, loss: 41.15873718261719\n",
            "epoch: 0, iter: 5000, loss: 42.991451263427734\n",
            "epoch: 0, iter: 5100, loss: 49.07178497314453\n",
            "epoch: 0, iter: 5200, loss: 39.85002517700195\n",
            "epoch: 0, iter: 5300, loss: 36.49831008911133\n",
            "epoch: 0, iter: 5400, loss: 37.84973907470703\n",
            "epoch: 0, iter: 5500, loss: 43.870914459228516\n",
            "epoch: 0, iter: 5600, loss: 41.2255973815918\n",
            "epoch: 0, iter: 5700, loss: 40.497825622558594\n",
            "epoch: 0, iter: 5800, loss: 46.306968688964844\n",
            "epoch: 0, iter: 5900, loss: 42.87832260131836\n",
            "epoch: 0, iter: 6000, loss: 44.4908447265625\n",
            "epoch: 0, iteration: 6000, simlex-999: SpearmanrResult(correlation=0.056474371644204685, pvalue=0.08158037320079391), men: SpearmanrResult(correlation=0.028547103832710165, pvalue=0.1535994546042063), sim353: SpearmanrResult(correlation=-0.009076101085984006, pvalue=0.8729334842063203), nearest to monster: ['monster', 'locations', 'covering', 'thrown', 'declined', 'banned', 'bound', 'wounded', 'adding', 'particle']\n",
            "\n",
            "epoch: 0, iter: 6100, loss: 40.8875846862793\n",
            "epoch: 0, iter: 6200, loss: 42.632747650146484\n",
            "epoch: 0, iter: 6300, loss: 38.20176315307617\n",
            "epoch: 0, iter: 6400, loss: 34.075687408447266\n",
            "epoch: 0, iter: 6500, loss: 37.02580642700195\n",
            "epoch: 0, iter: 6600, loss: 38.884307861328125\n",
            "epoch: 0, iter: 6700, loss: 34.16740417480469\n",
            "epoch: 0, iter: 6800, loss: 35.606529235839844\n",
            "epoch: 0, iter: 6900, loss: 38.067108154296875\n",
            "epoch: 0, iter: 7000, loss: 38.990806579589844\n",
            "epoch: 0, iter: 7100, loss: 36.87358856201172\n",
            "epoch: 0, iter: 7200, loss: 38.817596435546875\n",
            "epoch: 0, iter: 7300, loss: 40.30413818359375\n",
            "epoch: 0, iter: 7400, loss: 37.61985778808594\n",
            "epoch: 0, iter: 7500, loss: 35.49491500854492\n",
            "epoch: 0, iter: 7600, loss: 33.75785827636719\n",
            "epoch: 0, iter: 7700, loss: 44.318058013916016\n",
            "epoch: 0, iter: 7800, loss: 35.998451232910156\n",
            "epoch: 0, iter: 7900, loss: 42.51841735839844\n",
            "epoch: 0, iter: 8000, loss: 35.595306396484375\n",
            "epoch: 0, iteration: 8000, simlex-999: SpearmanrResult(correlation=0.06669671772893429, pvalue=0.03963994978889066), men: SpearmanrResult(correlation=0.03767997178528355, pvalue=0.059602143518073775), sim353: SpearmanrResult(correlation=-0.000543384199633011, pvalue=0.9923603836885238), nearest to monster: ['monster', 'covering', 'drop', 'sample', 'homes', 'damaged', 'themes', 'molecular', 'strings', 'wounded']\n",
            "\n",
            "epoch: 0, iter: 8100, loss: 35.166805267333984\n",
            "epoch: 0, iter: 8200, loss: 39.452484130859375\n",
            "epoch: 0, iter: 8300, loss: 36.044219970703125\n",
            "epoch: 0, iter: 8400, loss: 42.8715934753418\n",
            "epoch: 0, iter: 8500, loss: 35.42193603515625\n",
            "epoch: 0, iter: 8600, loss: 45.77275466918945\n",
            "epoch: 0, iter: 8700, loss: 34.321510314941406\n",
            "epoch: 0, iter: 8800, loss: 35.401405334472656\n",
            "epoch: 0, iter: 8900, loss: 36.80388259887695\n",
            "epoch: 0, iter: 9000, loss: 39.93381881713867\n",
            "epoch: 0, iter: 9100, loss: 34.906524658203125\n",
            "epoch: 0, iter: 9200, loss: 35.54966354370117\n",
            "epoch: 0, iter: 9300, loss: 36.4811897277832\n",
            "epoch: 0, iter: 9400, loss: 34.014625549316406\n",
            "epoch: 0, iter: 9500, loss: 37.099632263183594\n",
            "epoch: 0, iter: 9600, loss: 34.407989501953125\n",
            "epoch: 0, iter: 9700, loss: 33.395851135253906\n",
            "epoch: 0, iter: 9800, loss: 38.34513854980469\n",
            "epoch: 0, iter: 9900, loss: 36.01166534423828\n",
            "epoch: 0, iter: 10000, loss: 34.738216400146484\n",
            "epoch: 0, iteration: 10000, simlex-999: SpearmanrResult(correlation=0.06860418458166267, pvalue=0.034305657446823075), men: SpearmanrResult(correlation=0.045475469072667196, pvalue=0.02297725479168581), sim353: SpearmanrResult(correlation=0.015472654735967128, pvalue=0.7851143486336017), nearest to monster: ['monster', 'battleships', 'damaged', 'enjoyed', 'covering', 'underground', 'sample', 'furthermore', 'declined', 'drop']\n",
            "\n",
            "epoch: 0, iter: 10100, loss: 38.14243698120117\n",
            "epoch: 0, iter: 10200, loss: 34.402366638183594\n",
            "epoch: 0, iter: 10300, loss: 36.83221435546875\n",
            "epoch: 0, iter: 10400, loss: 35.506805419921875\n",
            "epoch: 0, iter: 10500, loss: 34.07112503051758\n",
            "epoch: 0, iter: 10600, loss: 37.94406509399414\n",
            "epoch: 0, iter: 10700, loss: 35.90528869628906\n",
            "epoch: 0, iter: 10800, loss: 34.69622039794922\n",
            "epoch: 0, iter: 10900, loss: 34.6096076965332\n",
            "epoch: 0, iter: 11000, loss: 35.929996490478516\n",
            "epoch: 0, iter: 11100, loss: 34.09210205078125\n",
            "epoch: 0, iter: 11200, loss: 35.537960052490234\n",
            "epoch: 0, iter: 11300, loss: 34.88569641113281\n",
            "epoch: 0, iter: 11400, loss: 35.5296630859375\n",
            "epoch: 0, iter: 11500, loss: 33.841064453125\n",
            "epoch: 0, iter: 11600, loss: 37.14215850830078\n",
            "epoch: 0, iter: 11700, loss: 35.91884994506836\n",
            "epoch: 0, iter: 11800, loss: 37.20144271850586\n",
            "epoch: 0, iter: 11900, loss: 33.97270965576172\n",
            "epoch: 0, iter: 12000, loss: 33.010154724121094\n",
            "epoch: 0, iteration: 12000, simlex-999: SpearmanrResult(correlation=0.0755222319046953, pvalue=0.019781435242130146), men: SpearmanrResult(correlation=0.049072582856577875, pvalue=0.01413239593200011), sim353: SpearmanrResult(correlation=0.023447311940159917, pvalue=0.6794436708874445), nearest to monster: ['monster', 'underground', 'enjoyed', 'integer', 'powered', 'tomb', 'wounded', 'simplest', 'installed', 'withdrawal']\n",
            "\n",
            "epoch: 0, iter: 12100, loss: 34.7757453918457\n",
            "epoch: 0, iter: 12200, loss: 34.076168060302734\n",
            "epoch: 0, iter: 12300, loss: 34.47975158691406\n",
            "epoch: 0, iter: 12400, loss: 33.22776412963867\n",
            "epoch: 0, iter: 12500, loss: 34.14054870605469\n",
            "epoch: 0, iter: 12600, loss: 33.535369873046875\n",
            "epoch: 0, iter: 12700, loss: 34.51906967163086\n",
            "epoch: 0, iter: 12800, loss: 34.118011474609375\n",
            "epoch: 0, iter: 12900, loss: 34.66770553588867\n",
            "epoch: 0, iter: 13000, loss: 32.98057174682617\n",
            "epoch: 0, iter: 13100, loss: 34.14727783203125\n",
            "epoch: 0, iter: 13200, loss: 37.041194915771484\n",
            "epoch: 0, iter: 13300, loss: 32.967529296875\n",
            "epoch: 0, iter: 13400, loss: 33.45307922363281\n",
            "epoch: 0, iter: 13500, loss: 32.46741485595703\n",
            "epoch: 0, iter: 13600, loss: 33.08403015136719\n",
            "epoch: 0, iter: 13700, loss: 33.7094612121582\n",
            "epoch: 0, iter: 13800, loss: 34.04615020751953\n",
            "epoch: 0, iter: 13900, loss: 32.830257415771484\n",
            "epoch: 0, iter: 14000, loss: 36.39988708496094\n",
            "epoch: 0, iteration: 14000, simlex-999: SpearmanrResult(correlation=0.0792059359591956, pvalue=0.01450571643233639), men: SpearmanrResult(correlation=0.0538131989981318, pvalue=0.007118175474538906), sim353: SpearmanrResult(correlation=0.028705439492388317, pvalue=0.612910394277714), nearest to monster: ['monster', 'damaged', 'earned', 'moves', 'powered', 'enjoyed', 'scheme', 'error', 'continuing', 'wounded']\n",
            "\n",
            "epoch: 0, iter: 14100, loss: 33.95890808105469\n",
            "epoch: 0, iter: 14200, loss: 33.405426025390625\n",
            "epoch: 0, iter: 14300, loss: 33.01673889160156\n",
            "epoch: 0, iter: 14400, loss: 32.699554443359375\n",
            "epoch: 0, iter: 14500, loss: 33.3966178894043\n",
            "epoch: 0, iter: 14600, loss: 32.729129791259766\n",
            "epoch: 0, iter: 14700, loss: 33.45749282836914\n",
            "epoch: 0, iter: 14800, loss: 33.610992431640625\n",
            "epoch: 0, iter: 14900, loss: 33.019954681396484\n",
            "epoch: 0, iter: 15000, loss: 32.43520736694336\n",
            "epoch: 0, iter: 15100, loss: 33.6442756652832\n",
            "epoch: 0, iter: 15200, loss: 33.25294494628906\n",
            "epoch: 0, iter: 15300, loss: 33.11228942871094\n",
            "epoch: 0, iter: 15400, loss: 33.19160842895508\n",
            "epoch: 0, iter: 15500, loss: 31.462032318115234\n",
            "epoch: 0, iter: 15600, loss: 34.136634826660156\n",
            "epoch: 0, iter: 15700, loss: 32.4737434387207\n",
            "epoch: 0, iter: 15800, loss: 32.521759033203125\n",
            "epoch: 0, iter: 15900, loss: 32.792484283447266\n",
            "epoch: 0, iter: 16000, loss: 32.40879821777344\n",
            "epoch: 0, iteration: 16000, simlex-999: SpearmanrResult(correlation=0.0801454549571615, pvalue=0.013376730375442828), men: SpearmanrResult(correlation=0.056578667539540134, pvalue=0.0046579232991922105), sim353: SpearmanrResult(correlation=0.030783683491056736, pvalue=0.587426889900857), nearest to monster: ['monster', 'moves', 'scheme', 'camera', 'owners', 'capability', 'reactions', 'tourist', 'noun', 'penalty']\n",
            "\n",
            "epoch: 0, iter: 16100, loss: 31.74828338623047\n",
            "epoch: 0, iter: 16200, loss: 33.16593933105469\n",
            "epoch: 0, iter: 16300, loss: 33.026512145996094\n",
            "epoch: 0, iter: 16400, loss: 33.951698303222656\n",
            "epoch: 0, iter: 16500, loss: 32.89470672607422\n",
            "epoch: 0, iter: 16600, loss: 32.735347747802734\n",
            "epoch: 0, iter: 16700, loss: 33.298431396484375\n",
            "epoch: 0, iter: 16800, loss: 31.999967575073242\n",
            "epoch: 0, iter: 16900, loss: 33.19562530517578\n",
            "epoch: 0, iter: 17000, loss: 32.78826904296875\n",
            "epoch: 0, iter: 17100, loss: 32.0982666015625\n",
            "epoch: 0, iter: 17200, loss: 32.73141860961914\n",
            "epoch: 0, iter: 17300, loss: 32.05320739746094\n",
            "epoch: 0, iter: 17400, loss: 32.96952819824219\n",
            "epoch: 0, iter: 17500, loss: 32.35126876831055\n",
            "epoch: 0, iter: 17600, loss: 32.03109359741211\n",
            "epoch: 0, iter: 17700, loss: 32.864646911621094\n",
            "epoch: 0, iter: 17800, loss: 32.30725860595703\n",
            "epoch: 0, iter: 17900, loss: 32.67045593261719\n",
            "epoch: 0, iter: 18000, loss: 32.41008377075195\n",
            "epoch: 0, iteration: 18000, simlex-999: SpearmanrResult(correlation=0.08314900471292454, pvalue=0.01027022868437687), men: SpearmanrResult(correlation=0.05703787013798571, pvalue=0.004333728075907701), sim353: SpearmanrResult(correlation=0.03413615071911305, pvalue=0.5473810138448203), nearest to monster: ['monster', 'noun', 'camera', 'calling', 'covering', 'owners', 'moves', 'tail', 'penalty', 'marks']\n",
            "\n",
            "epoch: 0, iter: 18100, loss: 34.1288948059082\n",
            "epoch: 0, iter: 18200, loss: 32.97951889038086\n",
            "epoch: 0, iter: 18300, loss: 32.98161315917969\n",
            "epoch: 0, iter: 18400, loss: 32.837955474853516\n",
            "epoch: 0, iter: 18500, loss: 33.378910064697266\n",
            "epoch: 0, iter: 18600, loss: 32.784385681152344\n",
            "epoch: 0, iter: 18700, loss: 33.16790771484375\n",
            "epoch: 0, iter: 18800, loss: 32.678924560546875\n",
            "epoch: 0, iter: 18900, loss: 31.859458923339844\n",
            "epoch: 0, iter: 19000, loss: 32.31916046142578\n",
            "epoch: 0, iter: 19100, loss: 32.18863296508789\n",
            "epoch: 0, iter: 19200, loss: 31.835554122924805\n",
            "epoch: 0, iter: 19300, loss: 32.93990707397461\n",
            "epoch: 0, iter: 19400, loss: 32.30719757080078\n",
            "epoch: 0, iter: 19500, loss: 32.21474838256836\n",
            "epoch: 0, iter: 19600, loss: 31.99469757080078\n",
            "epoch: 0, iter: 19700, loss: 32.28718948364258\n",
            "epoch: 0, iter: 19800, loss: 31.702119827270508\n",
            "epoch: 0, iter: 19900, loss: 33.25170135498047\n",
            "epoch: 0, iter: 20000, loss: 32.63927459716797\n",
            "epoch: 0, iteration: 20000, simlex-999: SpearmanrResult(correlation=0.0858405775957134, pvalue=0.008049635645036409), men: SpearmanrResult(correlation=0.05929770279009044, pvalue=0.0030168831758305022), sim353: SpearmanrResult(correlation=0.04180292038970287, pvalue=0.46116348475629854), nearest to monster: ['monster', 'noun', 'camera', 'scheme', 'server', 'calling', 'scoring', 'owners', 'wheel', 'earned']\n",
            "\n",
            "epoch: 0, iter: 20100, loss: 32.395572662353516\n",
            "epoch: 0, iter: 20200, loss: 32.796104431152344\n",
            "epoch: 0, iter: 20300, loss: 32.33261489868164\n",
            "epoch: 0, iter: 20400, loss: 32.31095886230469\n",
            "epoch: 0, iter: 20500, loss: 31.625211715698242\n",
            "epoch: 0, iter: 20600, loss: 31.475236892700195\n",
            "epoch: 0, iter: 20700, loss: 32.707298278808594\n",
            "epoch: 0, iter: 20800, loss: 32.152347564697266\n",
            "epoch: 0, iter: 20900, loss: 32.24357223510742\n",
            "epoch: 0, iter: 21000, loss: 31.841136932373047\n",
            "epoch: 0, iter: 21100, loss: 31.63398551940918\n",
            "epoch: 0, iter: 21200, loss: 32.28577423095703\n",
            "epoch: 0, iter: 21300, loss: 32.41093444824219\n",
            "epoch: 0, iter: 21400, loss: 31.565322875976562\n",
            "epoch: 0, iter: 21500, loss: 31.68331527709961\n",
            "epoch: 0, iter: 21600, loss: 32.18768310546875\n",
            "epoch: 0, iter: 21700, loss: 32.14234161376953\n",
            "epoch: 0, iter: 21800, loss: 32.16699981689453\n",
            "epoch: 0, iter: 21900, loss: 31.66488265991211\n",
            "epoch: 0, iter: 22000, loss: 31.847198486328125\n",
            "epoch: 0, iteration: 22000, simlex-999: SpearmanrResult(correlation=0.08404333835503505, pvalue=0.009478401429042601), men: SpearmanrResult(correlation=0.06253881831954015, pvalue=0.0017573712730899456), sim353: SpearmanrResult(correlation=0.04536778667839819, pvalue=0.42380427420176414), nearest to monster: ['monster', 'camera', 'scheme', 'noun', 'scoring', 'marks', 'server', 'capability', 'protocol', 'manufacturer']\n",
            "\n",
            "epoch: 0, iter: 22100, loss: 32.01354217529297\n",
            "epoch: 0, iter: 22200, loss: 32.45838928222656\n",
            "epoch: 0, iter: 22300, loss: 31.58429718017578\n",
            "epoch: 0, iter: 22400, loss: 32.19676971435547\n",
            "epoch: 0, iter: 22500, loss: 32.16627502441406\n",
            "epoch: 0, iter: 22600, loss: 31.317781448364258\n",
            "epoch: 0, iter: 22700, loss: 31.611623764038086\n",
            "epoch: 0, iter: 22800, loss: 32.18021774291992\n",
            "epoch: 0, iter: 22900, loss: 32.35356903076172\n",
            "epoch: 0, iter: 23000, loss: 32.7572135925293\n",
            "epoch: 0, iter: 23100, loss: 32.02249526977539\n",
            "epoch: 0, iter: 23200, loss: 31.92397117614746\n",
            "epoch: 0, iter: 23300, loss: 34.811241149902344\n",
            "epoch: 0, iter: 23400, loss: 31.655914306640625\n",
            "epoch: 0, iter: 23500, loss: 31.68148422241211\n",
            "epoch: 0, iter: 23600, loss: 32.36946487426758\n",
            "epoch: 0, iter: 23700, loss: 32.00149917602539\n",
            "epoch: 0, iter: 23800, loss: 32.48613739013672\n",
            "epoch: 0, iter: 23900, loss: 32.64463806152344\n",
            "epoch: 0, iter: 24000, loss: 32.5538444519043\n",
            "epoch: 0, iteration: 24000, simlex-999: SpearmanrResult(correlation=0.08562450611375359, pvalue=0.008210570183309562), men: SpearmanrResult(correlation=0.06411186041180054, pvalue=0.0013399199671547029), sim353: SpearmanrResult(correlation=0.05263802072138146, pvalue=0.35331455225833663), nearest to monster: ['monster', 'scheme', 'protocol', 'scoring', 'camera', 'noun', 'firm', 'marks', 'capability', 'narrow']\n",
            "\n",
            "epoch: 0, iter: 24100, loss: 32.18499755859375\n",
            "epoch: 0, iter: 24200, loss: 31.795372009277344\n",
            "epoch: 0, iter: 24300, loss: 31.82798194885254\n",
            "epoch: 0, iter: 24400, loss: 31.924314498901367\n",
            "epoch: 0, iter: 24500, loss: 31.860225677490234\n",
            "epoch: 0, iter: 24600, loss: 32.142127990722656\n",
            "epoch: 0, iter: 24700, loss: 31.819183349609375\n",
            "epoch: 0, iter: 24800, loss: 31.766212463378906\n",
            "epoch: 0, iter: 24900, loss: 32.58007049560547\n",
            "epoch: 0, iter: 25000, loss: 31.76087760925293\n",
            "epoch: 0, iter: 25100, loss: 32.27069091796875\n",
            "epoch: 0, iter: 25200, loss: 32.116172790527344\n",
            "epoch: 0, iter: 25300, loss: 31.487363815307617\n",
            "epoch: 0, iter: 25400, loss: 31.768474578857422\n",
            "epoch: 0, iter: 25500, loss: 32.063934326171875\n",
            "epoch: 0, iter: 25600, loss: 31.931148529052734\n",
            "epoch: 0, iter: 25700, loss: 31.28087615966797\n",
            "epoch: 0, iter: 25800, loss: 32.598655700683594\n",
            "epoch: 0, iter: 25900, loss: 31.602630615234375\n",
            "epoch: 0, iter: 26000, loss: 32.311527252197266\n",
            "epoch: 0, iteration: 26000, simlex-999: SpearmanrResult(correlation=0.08596373340708698, pvalue=0.007959173640013252), men: SpearmanrResult(correlation=0.06569380128940733, pvalue=0.0010140551604006856), sim353: SpearmanrResult(correlation=0.06253751421357925, pvalue=0.27000332135855726), nearest to monster: ['monster', 'protocol', 'underground', 'giant', 'calling', 'scoring', 'scheme', 'server', 'noun', 'illustrated']\n",
            "\n",
            "epoch: 0, iter: 26100, loss: 31.82002067565918\n",
            "epoch: 0, iter: 26200, loss: 31.894668579101562\n",
            "epoch: 0, iter: 26300, loss: 31.60350227355957\n",
            "epoch: 0, iter: 26400, loss: 32.4682502746582\n",
            "epoch: 0, iter: 26500, loss: 31.894752502441406\n",
            "epoch: 0, iter: 26600, loss: 32.04512023925781\n",
            "epoch: 0, iter: 26700, loss: 32.20697021484375\n",
            "epoch: 0, iter: 26800, loss: 32.28586196899414\n",
            "epoch: 0, iter: 26900, loss: 31.23895263671875\n",
            "epoch: 0, iter: 27000, loss: 31.221084594726562\n",
            "epoch: 0, iter: 27100, loss: 32.08095932006836\n",
            "epoch: 0, iter: 27200, loss: 31.38123893737793\n",
            "epoch: 0, iter: 27300, loss: 31.47606658935547\n",
            "epoch: 0, iter: 27400, loss: 31.861003875732422\n",
            "epoch: 0, iter: 27500, loss: 31.924009323120117\n",
            "epoch: 0, iter: 27600, loss: 31.89456558227539\n",
            "epoch: 0, iter: 27700, loss: 31.634593963623047\n",
            "epoch: 0, iter: 27800, loss: 32.006324768066406\n",
            "epoch: 0, iter: 27900, loss: 31.921207427978516\n",
            "epoch: 0, iter: 28000, loss: 31.493183135986328\n",
            "epoch: 0, iteration: 28000, simlex-999: SpearmanrResult(correlation=0.08563448542365543, pvalue=0.008203074531533239), men: SpearmanrResult(correlation=0.0671613339865574, pvalue=0.0007789126926349364), sim353: SpearmanrResult(correlation=0.06682608155364289, pvalue=0.23845312429819138), nearest to monster: ['monster', 'noun', 'protocol', 'giant', 'scheme', 'narrow', 'triple', 'wheel', 'calling', 'circuit']\n",
            "\n",
            "epoch: 0, iter: 28100, loss: 31.460391998291016\n",
            "epoch: 0, iter: 28200, loss: 32.03865432739258\n",
            "epoch: 0, iter: 28300, loss: 31.528209686279297\n",
            "epoch: 0, iter: 28400, loss: 31.993961334228516\n",
            "epoch: 0, iter: 28500, loss: 32.51808547973633\n",
            "epoch: 0, iter: 28600, loss: 31.39577865600586\n",
            "epoch: 0, iter: 28700, loss: 31.52813720703125\n",
            "epoch: 0, iter: 28800, loss: 31.422584533691406\n",
            "epoch: 0, iter: 28900, loss: 31.52276039123535\n",
            "epoch: 0, iter: 29000, loss: 31.314224243164062\n",
            "epoch: 0, iter: 29100, loss: 31.231096267700195\n",
            "epoch: 0, iter: 29200, loss: 31.739688873291016\n",
            "epoch: 0, iter: 29300, loss: 31.319318771362305\n",
            "epoch: 1, iter: 0, loss: 31.323366165161133\n",
            "epoch: 1, iteration: 0, simlex-999: SpearmanrResult(correlation=0.08639135523025923, pvalue=0.007652087102266428), men: SpearmanrResult(correlation=0.06831501730120537, pvalue=0.0006307473671582056), sim353: SpearmanrResult(correlation=0.06842883991849488, pvalue=0.22735452840088494), nearest to monster: ['monster', 'noun', 'giant', 'wheel', 'protocol', 'narrow', 'scheme', 'scoring', 'circuit', 'marks']\n",
            "\n",
            "epoch: 1, iter: 100, loss: 31.833642959594727\n",
            "epoch: 1, iter: 200, loss: 31.771045684814453\n",
            "epoch: 1, iter: 300, loss: 31.50340461730957\n",
            "epoch: 1, iter: 400, loss: 31.9205379486084\n",
            "epoch: 1, iter: 500, loss: 31.31815528869629\n",
            "epoch: 1, iter: 600, loss: 31.388822555541992\n",
            "epoch: 1, iter: 700, loss: 31.48935890197754\n",
            "epoch: 1, iter: 800, loss: 31.430015563964844\n",
            "epoch: 1, iter: 900, loss: 32.05698776245117\n",
            "epoch: 1, iter: 1000, loss: 31.55906105041504\n",
            "epoch: 1, iter: 1100, loss: 31.472082138061523\n",
            "epoch: 1, iter: 1200, loss: 31.554824829101562\n",
            "epoch: 1, iter: 1300, loss: 31.273662567138672\n",
            "epoch: 1, iter: 1400, loss: 31.69467544555664\n",
            "epoch: 1, iter: 1500, loss: 30.83802032470703\n",
            "epoch: 1, iter: 1600, loss: 32.24837875366211\n",
            "epoch: 1, iter: 1700, loss: 31.236289978027344\n",
            "epoch: 1, iter: 1800, loss: 31.81583023071289\n",
            "epoch: 1, iter: 1900, loss: 31.85976791381836\n",
            "epoch: 1, iter: 2000, loss: 31.369979858398438\n",
            "epoch: 1, iteration: 2000, simlex-999: SpearmanrResult(correlation=0.08757521432464897, pvalue=0.006856488999301858), men: SpearmanrResult(correlation=0.069551600826957, pvalue=0.0005013097863989423), sim353: SpearmanrResult(correlation=0.07238711106519138, pvalue=0.20152777344705644), nearest to monster: ['monster', 'noun', 'giant', 'protocol', 'scoring', 'wheel', 'calling', 'marks', 'carries', 'neck']\n",
            "\n",
            "epoch: 1, iter: 2100, loss: 31.71782684326172\n",
            "epoch: 1, iter: 2200, loss: 32.46800994873047\n",
            "epoch: 1, iter: 2300, loss: 31.930604934692383\n",
            "epoch: 1, iter: 2400, loss: 32.061649322509766\n",
            "epoch: 1, iter: 2500, loss: 31.35696029663086\n",
            "epoch: 1, iter: 2600, loss: 31.726537704467773\n",
            "epoch: 1, iter: 2700, loss: 31.79720687866211\n",
            "epoch: 1, iter: 2800, loss: 31.33445930480957\n",
            "epoch: 1, iter: 2900, loss: 31.74420928955078\n",
            "epoch: 1, iter: 3000, loss: 31.490365982055664\n",
            "epoch: 1, iter: 3100, loss: 31.85082244873047\n",
            "epoch: 1, iter: 3200, loss: 31.769805908203125\n",
            "epoch: 1, iter: 3300, loss: 31.598289489746094\n",
            "epoch: 1, iter: 3400, loss: 31.577198028564453\n",
            "epoch: 1, iter: 3500, loss: 30.82488250732422\n",
            "epoch: 1, iter: 3600, loss: 31.745014190673828\n",
            "epoch: 1, iter: 3700, loss: 31.1189022064209\n",
            "epoch: 1, iter: 3800, loss: 31.92327117919922\n",
            "epoch: 1, iter: 3900, loss: 31.215837478637695\n",
            "epoch: 1, iter: 4000, loss: 31.189512252807617\n",
            "epoch: 1, iteration: 4000, simlex-999: SpearmanrResult(correlation=0.08863971492201679, pvalue=0.006205337087045603), men: SpearmanrResult(correlation=0.07090656890165163, pvalue=0.000388136222520682), sim353: SpearmanrResult(correlation=0.07363002659852552, pvalue=0.19387515722224863), nearest to monster: ['monster', 'noun', 'protocol', 'operator', 'scoring', 'giant', 'camera', 'carries', 'neck', 'wheel']\n",
            "\n",
            "epoch: 1, iter: 4100, loss: 31.807666778564453\n",
            "epoch: 1, iter: 4200, loss: 31.42727279663086\n",
            "epoch: 1, iter: 4300, loss: 31.6743221282959\n",
            "epoch: 1, iter: 4400, loss: 31.238685607910156\n",
            "epoch: 1, iter: 4500, loss: 30.882936477661133\n",
            "epoch: 1, iter: 4600, loss: 31.14011001586914\n",
            "epoch: 1, iter: 4700, loss: 31.69738006591797\n",
            "epoch: 1, iter: 4800, loss: 31.724611282348633\n",
            "epoch: 1, iter: 4900, loss: 31.42301368713379\n",
            "epoch: 1, iter: 5000, loss: 31.49811553955078\n",
            "epoch: 1, iter: 5100, loss: 31.451387405395508\n",
            "epoch: 1, iter: 5200, loss: 30.695720672607422\n",
            "epoch: 1, iter: 5300, loss: 31.52522087097168\n",
            "epoch: 1, iter: 5400, loss: 31.70336151123047\n",
            "epoch: 1, iter: 5500, loss: 31.514925003051758\n",
            "epoch: 1, iter: 5600, loss: 31.660446166992188\n",
            "epoch: 1, iter: 5700, loss: 31.544330596923828\n",
            "epoch: 1, iter: 5800, loss: 31.10457420349121\n",
            "epoch: 1, iter: 5900, loss: 31.158103942871094\n",
            "epoch: 1, iter: 6000, loss: 31.63458824157715\n",
            "epoch: 1, iteration: 6000, simlex-999: SpearmanrResult(correlation=0.0892600834556715, pvalue=0.00585202661147631), men: SpearmanrResult(correlation=0.07395170103299649, pvalue=0.0002149175315243936), sim353: SpearmanrResult(correlation=0.07831326658196681, pvalue=0.16694615520871198), nearest to monster: ['monster', 'protocol', 'noun', 'scoring', 'camera', 'carries', 'wheel', 'neck', 'giant', 'eternal']\n",
            "\n",
            "epoch: 1, iter: 6100, loss: 31.387136459350586\n",
            "epoch: 1, iter: 6200, loss: 31.94541358947754\n",
            "epoch: 1, iter: 6300, loss: 31.1632137298584\n",
            "epoch: 1, iter: 6400, loss: 30.920751571655273\n",
            "epoch: 1, iter: 6500, loss: 31.623838424682617\n",
            "epoch: 1, iter: 6600, loss: 31.50497055053711\n",
            "epoch: 1, iter: 6700, loss: 31.637453079223633\n",
            "epoch: 1, iter: 6800, loss: 31.07155990600586\n",
            "epoch: 1, iter: 6900, loss: 31.185060501098633\n",
            "epoch: 1, iter: 7000, loss: 31.507814407348633\n",
            "epoch: 1, iter: 7100, loss: 31.24095916748047\n",
            "epoch: 1, iter: 7200, loss: 31.21225357055664\n",
            "epoch: 1, iter: 7300, loss: 31.485326766967773\n",
            "epoch: 1, iter: 7400, loss: 31.125076293945312\n",
            "epoch: 1, iter: 7500, loss: 30.985403060913086\n",
            "epoch: 1, iter: 7600, loss: 30.942764282226562\n",
            "epoch: 1, iter: 7700, loss: 31.784549713134766\n",
            "epoch: 1, iter: 7800, loss: 31.76082992553711\n",
            "epoch: 1, iter: 7900, loss: 31.341705322265625\n",
            "epoch: 1, iter: 8000, loss: 31.210355758666992\n",
            "epoch: 1, iteration: 8000, simlex-999: SpearmanrResult(correlation=0.09310539005349885, pvalue=0.004037804374380043), men: SpearmanrResult(correlation=0.07518571577614852, pvalue=0.00016806709825485563), sim353: SpearmanrResult(correlation=0.08451101636200857, pvalue=0.13574097572154556), nearest to monster: ['monster', 'protocol', 'scoring', 'noun', 'wheel', 'giant', 'camera', 'operator', 'eternal', 'keyboard']\n",
            "\n",
            "epoch: 1, iter: 8100, loss: 31.10982322692871\n",
            "epoch: 1, iter: 8200, loss: 31.4071044921875\n",
            "epoch: 1, iter: 8300, loss: 31.400245666503906\n",
            "epoch: 1, iter: 8400, loss: 31.059741973876953\n",
            "epoch: 1, iter: 8500, loss: 31.80914306640625\n",
            "epoch: 1, iter: 8600, loss: 31.68463134765625\n",
            "epoch: 1, iter: 8700, loss: 31.523332595825195\n",
            "epoch: 1, iter: 8800, loss: 31.068012237548828\n",
            "epoch: 1, iter: 8900, loss: 31.091161727905273\n",
            "epoch: 1, iter: 9000, loss: 31.05225944519043\n",
            "epoch: 1, iter: 9100, loss: 31.35640525817871\n",
            "epoch: 1, iter: 9200, loss: 30.907636642456055\n",
            "epoch: 1, iter: 9300, loss: 30.865739822387695\n",
            "epoch: 1, iter: 9400, loss: 31.506345748901367\n",
            "epoch: 1, iter: 9500, loss: 32.07880783081055\n",
            "epoch: 1, iter: 9600, loss: 30.938390731811523\n",
            "epoch: 1, iter: 9700, loss: 31.760255813598633\n",
            "epoch: 1, iter: 9800, loss: 31.521902084350586\n",
            "epoch: 1, iter: 9900, loss: 31.317811965942383\n",
            "epoch: 1, iter: 10000, loss: 30.790433883666992\n",
            "epoch: 1, iteration: 10000, simlex-999: SpearmanrResult(correlation=0.0943230915500362, pvalue=0.0035801531864151173), men: SpearmanrResult(correlation=0.07561657898235528, pvalue=0.0001541065192251729), sim353: SpearmanrResult(correlation=0.08428579662352187, pvalue=0.13679032210751868), nearest to monster: ['monster', 'protocol', 'scoring', 'wheel', 'giant', 'camera', 'indicating', 'eternal', 'noun', 'carries']\n",
            "\n",
            "epoch: 1, iter: 10100, loss: 31.171707153320312\n",
            "epoch: 1, iter: 10200, loss: 31.493017196655273\n",
            "epoch: 1, iter: 10300, loss: 31.82160186767578\n",
            "epoch: 1, iter: 10400, loss: 31.31926727294922\n",
            "epoch: 1, iter: 10500, loss: 31.046222686767578\n",
            "epoch: 1, iter: 10600, loss: 31.12728500366211\n",
            "epoch: 1, iter: 10700, loss: 31.01797866821289\n",
            "epoch: 1, iter: 10800, loss: 31.159528732299805\n",
            "epoch: 1, iter: 10900, loss: 30.989831924438477\n",
            "epoch: 1, iter: 11000, loss: 30.944223403930664\n",
            "epoch: 1, iter: 11100, loss: 31.75858497619629\n",
            "epoch: 1, iter: 11200, loss: 31.346534729003906\n",
            "epoch: 1, iter: 11300, loss: 31.0003719329834\n",
            "epoch: 1, iter: 11400, loss: 31.741788864135742\n",
            "epoch: 1, iter: 11500, loss: 31.046985626220703\n",
            "epoch: 1, iter: 11600, loss: 30.75389289855957\n",
            "epoch: 1, iter: 11700, loss: 31.447219848632812\n",
            "epoch: 1, iter: 11800, loss: 31.5010986328125\n",
            "epoch: 1, iter: 11900, loss: 31.19786834716797\n",
            "epoch: 1, iter: 12000, loss: 31.3983154296875\n",
            "epoch: 1, iteration: 12000, simlex-999: SpearmanrResult(correlation=0.09675953866045194, pvalue=0.0028029601345331656), men: SpearmanrResult(correlation=0.07639813894270316, pvalue=0.00013152494014987963), sim353: SpearmanrResult(correlation=0.08775566771177977, pvalue=0.12130122798015153), nearest to monster: ['monster', 'protocol', 'scoring', 'noun', 'wheel', 'operator', 'eternal', 'alien', 'narrow', 'keyboard']\n",
            "\n",
            "epoch: 1, iter: 12100, loss: 31.624753952026367\n",
            "epoch: 1, iter: 12200, loss: 31.195526123046875\n",
            "epoch: 1, iter: 12300, loss: 31.63898468017578\n",
            "epoch: 1, iter: 12400, loss: 31.87898826599121\n",
            "epoch: 1, iter: 12500, loss: 31.484500885009766\n",
            "epoch: 1, iter: 12600, loss: 31.69540023803711\n",
            "epoch: 1, iter: 12700, loss: 31.412216186523438\n",
            "epoch: 1, iter: 12800, loss: 31.168336868286133\n",
            "epoch: 1, iter: 12900, loss: 31.481613159179688\n",
            "epoch: 1, iter: 13000, loss: 31.648239135742188\n",
            "epoch: 1, iter: 13100, loss: 31.480159759521484\n",
            "epoch: 1, iter: 13200, loss: 30.678024291992188\n",
            "epoch: 1, iter: 13300, loss: 31.630233764648438\n",
            "epoch: 1, iter: 13400, loss: 31.363304138183594\n",
            "epoch: 1, iter: 13500, loss: 31.74050521850586\n",
            "epoch: 1, iter: 13600, loss: 31.57128143310547\n",
            "epoch: 1, iter: 13700, loss: 30.844942092895508\n",
            "epoch: 1, iter: 13800, loss: 31.17472267150879\n",
            "epoch: 1, iter: 13900, loss: 31.60284423828125\n",
            "epoch: 1, iter: 14000, loss: 31.825069427490234\n",
            "epoch: 1, iteration: 14000, simlex-999: SpearmanrResult(correlation=0.09827904994225212, pvalue=0.002399633278715888), men: SpearmanrResult(correlation=0.07929887461639062, pvalue=7.211058186266025e-05), sim353: SpearmanrResult(correlation=0.09200666581657889, pvalue=0.10422665484963951), nearest to monster: ['monster', 'scoring', 'protocol', 'noun', 'giant', 'sasquatch', 'carries', 'keyboard', 'alien', 'calling']\n",
            "\n",
            "epoch: 1, iter: 14100, loss: 31.241857528686523\n",
            "epoch: 1, iter: 14200, loss: 30.829071044921875\n",
            "epoch: 1, iter: 14300, loss: 31.178447723388672\n",
            "epoch: 1, iter: 14400, loss: 31.250577926635742\n",
            "epoch: 1, iter: 14500, loss: 31.788665771484375\n",
            "epoch: 1, iter: 14600, loss: 31.45791244506836\n",
            "epoch: 1, iter: 14700, loss: 31.600276947021484\n",
            "epoch: 1, iter: 14800, loss: 31.265289306640625\n",
            "epoch: 1, iter: 14900, loss: 30.861242294311523\n",
            "epoch: 1, iter: 15000, loss: 31.44761085510254\n",
            "epoch: 1, iter: 15100, loss: 31.382389068603516\n",
            "epoch: 1, iter: 15200, loss: 31.721107482910156\n",
            "epoch: 1, iter: 15300, loss: 31.432920455932617\n",
            "epoch: 1, iter: 15400, loss: 31.134065628051758\n",
            "epoch: 1, iter: 15500, loss: 31.306732177734375\n",
            "epoch: 1, iter: 15600, loss: 31.134113311767578\n",
            "epoch: 1, iter: 15700, loss: 31.4090633392334\n",
            "epoch: 1, iter: 15800, loss: 31.272687911987305\n",
            "epoch: 1, iter: 15900, loss: 31.208099365234375\n",
            "epoch: 1, iter: 16000, loss: 30.6237735748291\n",
            "epoch: 1, iteration: 16000, simlex-999: SpearmanrResult(correlation=0.09904236318077402, pvalue=0.002217716340348032), men: SpearmanrResult(correlation=0.08094390114794096, pvalue=5.08205969732388e-05), sim353: SpearmanrResult(correlation=0.09923306908508692, pvalue=0.07961625378614234), nearest to monster: ['monster', 'protocol', 'noun', 'scoring', 'giant', 'triple', 'sasquatch', 'carries', 'keyboard', 'atheist']\n",
            "\n",
            "epoch: 1, iter: 16100, loss: 30.787151336669922\n",
            "epoch: 1, iter: 16200, loss: 30.64605712890625\n",
            "epoch: 1, iter: 16300, loss: 31.324432373046875\n",
            "epoch: 1, iter: 16400, loss: 30.982742309570312\n",
            "epoch: 1, iter: 16500, loss: 31.36542510986328\n",
            "epoch: 1, iter: 16600, loss: 30.741561889648438\n",
            "epoch: 1, iter: 16700, loss: 31.20665740966797\n",
            "epoch: 1, iter: 16800, loss: 31.370187759399414\n",
            "epoch: 1, iter: 16900, loss: 31.760028839111328\n",
            "epoch: 1, iter: 17000, loss: 31.09383773803711\n",
            "epoch: 1, iter: 17100, loss: 31.576696395874023\n",
            "epoch: 1, iter: 17200, loss: 31.53241539001465\n",
            "epoch: 1, iter: 17300, loss: 31.396556854248047\n",
            "epoch: 1, iter: 17400, loss: 31.279069900512695\n",
            "epoch: 1, iter: 17500, loss: 30.943702697753906\n",
            "epoch: 1, iter: 17600, loss: 30.73080062866211\n",
            "epoch: 1, iter: 17700, loss: 31.148914337158203\n",
            "epoch: 1, iter: 17800, loss: 31.06964111328125\n",
            "epoch: 1, iter: 17900, loss: 31.038684844970703\n",
            "epoch: 1, iter: 18000, loss: 31.5753173828125\n",
            "epoch: 1, iteration: 18000, simlex-999: SpearmanrResult(correlation=0.10265903895734868, pvalue=0.0015154182023110062), men: SpearmanrResult(correlation=0.08248673690675482, pvalue=3.638496629186561e-05), sim353: SpearmanrResult(correlation=0.10271194113596026, pvalue=0.06957132198707175), nearest to monster: ['monster', 'protocol', 'noun', 'scoring', 'giant', 'carries', 'circuit', 'atheist', 'pen', 'triple']\n",
            "\n",
            "epoch: 1, iter: 18100, loss: 31.378101348876953\n",
            "epoch: 1, iter: 18200, loss: 31.18499755859375\n",
            "epoch: 1, iter: 18300, loss: 31.094566345214844\n",
            "epoch: 1, iter: 18400, loss: 31.03169059753418\n",
            "epoch: 1, iter: 18500, loss: 30.89459228515625\n",
            "epoch: 1, iter: 18600, loss: 31.432315826416016\n",
            "epoch: 1, iter: 18700, loss: 31.108009338378906\n",
            "epoch: 1, iter: 18800, loss: 31.11566925048828\n",
            "epoch: 1, iter: 18900, loss: 31.39504051208496\n",
            "epoch: 1, iter: 19000, loss: 31.571760177612305\n",
            "epoch: 1, iter: 19100, loss: 31.46836280822754\n",
            "epoch: 1, iter: 19200, loss: 30.675273895263672\n",
            "epoch: 1, iter: 19300, loss: 31.321338653564453\n",
            "epoch: 1, iter: 19400, loss: 31.232784271240234\n",
            "epoch: 1, iter: 19500, loss: 31.132925033569336\n",
            "epoch: 1, iter: 19600, loss: 31.48595428466797\n",
            "epoch: 1, iter: 19700, loss: 31.315162658691406\n",
            "epoch: 1, iter: 19800, loss: 31.51089859008789\n",
            "epoch: 1, iter: 19900, loss: 31.07308006286621\n",
            "epoch: 1, iter: 20000, loss: 31.82876968383789\n",
            "epoch: 1, iteration: 20000, simlex-999: SpearmanrResult(correlation=0.10314261092303864, pvalue=0.0014388858203121702), men: SpearmanrResult(correlation=0.08342107899139062, pvalue=2.963558074416606e-05), sim353: SpearmanrResult(correlation=0.10251450697772124, pvalue=0.07011224586404081), nearest to monster: ['monster', 'protocol', 'noun', 'giant', 'pen', 'circuit', 'carries', 'alien', 'curve', 'wheel']\n",
            "\n",
            "epoch: 1, iter: 20100, loss: 30.914871215820312\n",
            "epoch: 1, iter: 20200, loss: 30.5980281829834\n",
            "epoch: 1, iter: 20300, loss: 31.34394645690918\n",
            "epoch: 1, iter: 20400, loss: 31.360660552978516\n",
            "epoch: 1, iter: 20500, loss: 31.267866134643555\n",
            "epoch: 1, iter: 20600, loss: 31.108474731445312\n",
            "epoch: 1, iter: 20700, loss: 30.7585506439209\n",
            "epoch: 1, iter: 20800, loss: 31.154752731323242\n",
            "epoch: 1, iter: 20900, loss: 30.722095489501953\n",
            "epoch: 1, iter: 21000, loss: 31.443073272705078\n",
            "epoch: 1, iter: 21100, loss: 30.995250701904297\n",
            "epoch: 1, iter: 21200, loss: 31.14746856689453\n",
            "epoch: 1, iter: 21300, loss: 31.298070907592773\n",
            "epoch: 1, iter: 21400, loss: 31.07059097290039\n",
            "epoch: 1, iter: 21500, loss: 30.84886932373047\n",
            "epoch: 1, iter: 21600, loss: 31.193294525146484\n",
            "epoch: 1, iter: 21700, loss: 31.33860969543457\n",
            "epoch: 1, iter: 21800, loss: 30.33837890625\n",
            "epoch: 1, iter: 21900, loss: 30.53495979309082\n",
            "epoch: 1, iter: 22000, loss: 30.732059478759766\n",
            "epoch: 1, iteration: 22000, simlex-999: SpearmanrResult(correlation=0.10384555109191107, pvalue=0.001333963629600743), men: SpearmanrResult(correlation=0.08323142056387753, pvalue=3.0901251660090676e-05), sim353: SpearmanrResult(correlation=0.1027285733494888, pvalue=0.06952590973397861), nearest to monster: ['monster', 'protocol', 'giant', 'noun', 'curve', 'camera', 'carries', 'operator', 'pen', 'keyboard']\n",
            "\n",
            "epoch: 1, iter: 22100, loss: 31.340768814086914\n",
            "epoch: 1, iter: 22200, loss: 31.174774169921875\n",
            "epoch: 1, iter: 22300, loss: 31.11911964416504\n",
            "epoch: 1, iter: 22400, loss: 31.782560348510742\n",
            "epoch: 1, iter: 22500, loss: 30.891761779785156\n",
            "epoch: 1, iter: 22600, loss: 31.103361129760742\n",
            "epoch: 1, iter: 22700, loss: 30.87161636352539\n",
            "epoch: 1, iter: 22800, loss: 31.349605560302734\n",
            "epoch: 1, iter: 22900, loss: 31.13916778564453\n",
            "epoch: 1, iter: 23000, loss: 30.798316955566406\n",
            "epoch: 1, iter: 23100, loss: 31.59273910522461\n",
            "epoch: 1, iter: 23200, loss: 31.329893112182617\n",
            "epoch: 1, iter: 23300, loss: 31.055654525756836\n",
            "epoch: 1, iter: 23400, loss: 30.740236282348633\n",
            "epoch: 1, iter: 23500, loss: 31.283489227294922\n",
            "epoch: 1, iter: 23600, loss: 31.457202911376953\n",
            "epoch: 1, iter: 23700, loss: 31.04062271118164\n",
            "epoch: 1, iter: 23800, loss: 31.501766204833984\n",
            "epoch: 1, iter: 23900, loss: 31.17255973815918\n",
            "epoch: 1, iter: 24000, loss: 31.008119583129883\n",
            "epoch: 1, iteration: 24000, simlex-999: SpearmanrResult(correlation=0.10522028970201162, pvalue=0.0011488551429947895), men: SpearmanrResult(correlation=0.08487273104227794, pvalue=2.145518223052986e-05), sim353: SpearmanrResult(correlation=0.1030721753136788, pvalue=0.06859315263398301), nearest to monster: ['monster', 'protocol', 'noun', 'giant', 'carries', 'camera', 'curve', 'pen', 'keyboard', 'scheme']\n",
            "\n",
            "epoch: 1, iter: 24100, loss: 30.91338539123535\n",
            "epoch: 1, iter: 24200, loss: 30.762115478515625\n",
            "epoch: 1, iter: 24300, loss: 31.296092987060547\n",
            "epoch: 1, iter: 24400, loss: 31.240909576416016\n",
            "epoch: 1, iter: 24500, loss: 31.16200828552246\n",
            "epoch: 1, iter: 24600, loss: 30.893632888793945\n",
            "epoch: 1, iter: 24700, loss: 31.241744995117188\n",
            "epoch: 1, iter: 24800, loss: 30.647401809692383\n",
            "epoch: 1, iter: 24900, loss: 31.38899803161621\n",
            "epoch: 1, iter: 25000, loss: 31.394777297973633\n",
            "epoch: 1, iter: 25100, loss: 30.573444366455078\n",
            "epoch: 1, iter: 25200, loss: 30.858896255493164\n",
            "epoch: 1, iter: 25300, loss: 30.877077102661133\n",
            "epoch: 1, iter: 25400, loss: 31.28232765197754\n",
            "epoch: 1, iter: 25500, loss: 30.708330154418945\n",
            "epoch: 1, iter: 25600, loss: 31.04726791381836\n",
            "epoch: 1, iter: 25700, loss: 31.675994873046875\n",
            "epoch: 1, iter: 25800, loss: 31.17116928100586\n",
            "epoch: 1, iter: 25900, loss: 31.627225875854492\n",
            "epoch: 1, iter: 26000, loss: 30.777585983276367\n",
            "epoch: 1, iteration: 26000, simlex-999: SpearmanrResult(correlation=0.10737325416669198, pvalue=0.0009060210634536081), men: SpearmanrResult(correlation=0.08556608972308416, pvalue=1.835436426394087e-05), sim353: SpearmanrResult(correlation=0.10902259429489587, pvalue=0.0540000008002514), nearest to monster: ['monster', 'protocol', 'noun', 'giant', 'camera', 'pen', 'carries', 'scheme', 'syllable', 'keyboard']\n",
            "\n",
            "epoch: 1, iter: 26100, loss: 31.32539939880371\n",
            "epoch: 1, iter: 26200, loss: 30.578359603881836\n",
            "epoch: 1, iter: 26300, loss: 31.065263748168945\n",
            "epoch: 1, iter: 26400, loss: 31.00471305847168\n",
            "epoch: 1, iter: 26500, loss: 31.226518630981445\n",
            "epoch: 1, iter: 26600, loss: 30.5660457611084\n",
            "epoch: 1, iter: 26700, loss: 31.31158447265625\n",
            "epoch: 1, iter: 26800, loss: 31.471385955810547\n",
            "epoch: 1, iter: 26900, loss: 31.41477394104004\n",
            "epoch: 1, iter: 27000, loss: 31.333736419677734\n",
            "epoch: 1, iter: 27100, loss: 31.11928367614746\n",
            "epoch: 1, iter: 27200, loss: 30.891067504882812\n",
            "epoch: 1, iter: 27300, loss: 30.91944694519043\n",
            "epoch: 1, iter: 27400, loss: 31.019899368286133\n",
            "epoch: 1, iter: 27500, loss: 30.653797149658203\n",
            "epoch: 1, iter: 27600, loss: 30.98436737060547\n",
            "epoch: 1, iter: 27700, loss: 31.2142276763916\n",
            "epoch: 1, iter: 27800, loss: 31.268024444580078\n",
            "epoch: 1, iter: 27900, loss: 30.74524688720703\n",
            "epoch: 1, iter: 28000, loss: 30.808738708496094\n",
            "epoch: 1, iteration: 28000, simlex-999: SpearmanrResult(correlation=0.10928503959185873, pvalue=0.0007311444355773441), men: SpearmanrResult(correlation=0.0860783913943253, pvalue=1.6342590211968197e-05), sim353: SpearmanrResult(correlation=0.11401519344996836, pvalue=0.04383747403217389), nearest to monster: ['monster', 'noun', 'protocol', 'giant', 'camera', 'scheme', 'syllable', 'pen', 'curve', 'operator']\n",
            "\n",
            "epoch: 1, iter: 28100, loss: 31.03307342529297\n",
            "epoch: 1, iter: 28200, loss: 31.333707809448242\n",
            "epoch: 1, iter: 28300, loss: 31.361255645751953\n",
            "epoch: 1, iter: 28400, loss: 30.85529327392578\n",
            "epoch: 1, iter: 28500, loss: 30.728710174560547\n",
            "epoch: 1, iter: 28600, loss: 30.937597274780273\n",
            "epoch: 1, iter: 28700, loss: 31.330909729003906\n",
            "epoch: 1, iter: 28800, loss: 31.10799789428711\n",
            "epoch: 1, iter: 28900, loss: 31.20931625366211\n",
            "epoch: 1, iter: 29000, loss: 31.573259353637695\n",
            "epoch: 1, iter: 29100, loss: 30.839393615722656\n",
            "epoch: 1, iter: 29200, loss: 31.222017288208008\n",
            "epoch: 1, iter: 29300, loss: 31.168479919433594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CNlNYWe4_ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load(\"embedding-{}.th\".format(EMBEDDING_SIZE)))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5CB9bng-a3M",
        "colab_type": "text"
      },
      "source": [
        "# 2. 在 MEN 和 Simplex-999 数据集上做评估"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfqZshNt4_MX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_weights = model.input_embeddings()\n",
        "print(\"simlex-999\", evaluate(\"simlex-999.txt\", embedding_weights))\n",
        "print(\"men\", evaluate(\"men.txt\", embedding_weights))\n",
        "print(\"wordsim353\", evaluate(\"wordsim353.csv\", embedding_weights))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtsRTZd9-gwZ",
        "colab_type": "text"
      },
      "source": [
        "# 3. 寻找nearest neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3gsBOeF4_KH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "6946933c-b46c-422d-fbab-af09a0fdfe7e"
      },
      "source": [
        "for word in [\"good\", \"fresh\", \"monster\", \"green\", \"like\", \"america\", \"chicago\", \"work\", \"computer\", \"language\"]:\n",
        "    print(word, find_nearest(word))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good ['good', 'strong', 'software', 'free', 'better', 'low', 'relatively', 'simple', 'special', 'individual']\n",
            "fresh ['fresh', 'oral', 'uniform', 'mechanical', 'noise', 'evolutionary', 'marketing', 'freight', 'ammunition', 'reasoning']\n",
            "monster ['monster', 'noun', 'protocol', 'giant', 'scheme', 'curve', 'operator', 'pen', 'camera', 'rifle']\n",
            "green ['green', 'plant', 'dark', 'ice', 'bass', 'audio', 'mountain', 'deep', 'pro', 'oil']\n",
            "like ['like', 'non', 'using', 'without', 'body', 'cell', 'animal', 'include', 'good', 'human']\n",
            "america ['america', 'africa', 'australia', 'europe', 'asia', 'canada', 'india', 'germany', 'middle', 'union']\n",
            "chicago ['chicago', 'sweden', 'poland', 'los', 'francisco', 'virginia', 'georgia', 'victoria', 'hungary', 'texas']\n",
            "work ['work', 'life', 'death', 'position', 'upon', 'works', 'body', 'family', 'father', 'name']\n",
            "computer ['computer', 'standard', 'big', 'video', 'space', 'special', 'basic', 'science', 'historical', 'text']\n",
            "language ['language', 'art', 'modern', 'arabic', 'historical', 'word', 'culture', 'ancient', 'science', 'greek']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSYpw-nj-lIv",
        "colab_type": "text"
      },
      "source": [
        "# 4. 单词之间的关系"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcObngK84_HA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "e92c347e-5ece-4320-f09b-eff2bd2afebf"
      },
      "source": [
        "man_idx = word_to_idx[\"man\"] \n",
        "king_idx = word_to_idx[\"king\"] \n",
        "woman_idx = word_to_idx[\"woman\"]\n",
        "embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]\n",
        "cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
        "for i in cos_dis.argsort()[:20]:\n",
        "    print(idx_to_word[i])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "charles\n",
            "king\n",
            "james\n",
            "henry\n",
            "david\n",
            "pope\n",
            "william\n",
            "louis\n",
            "iii\n",
            "albert\n",
            "george\n",
            "iv\n",
            "paul\n",
            "emperor\n",
            "peter\n",
            "thomas\n",
            "joseph\n",
            "john\n",
            "president\n",
            "sir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYVgu5go-nkD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}